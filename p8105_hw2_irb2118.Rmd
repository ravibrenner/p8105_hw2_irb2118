---
title: "P8105 - HW2"
output: github_document
date: "Due: 2024-10-02"
---

Author: Ravi Brenner (irb2118)

# Introduction
This Rmarkdown document is for P8105 - Data Science 1, homework 2.

# Methods
The data for this homework comes from the course website, but all files can be found in the `data/` folder. Additionally, we will be using packages from the `tidyverse` and `readxl`.

```{r, message = FALSE}
library(tidyverse)
library(readxl)
```


# Problems

## Problem 1

This problem is about the NYC subway system.

First, we must read in the data, clean up the column names, and select and transform the columns that we need:

```{r}
subway_df <-
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |>
  janitor::clean_names() |>
  select(
    line,
    station_name,
    station_latitude,
    station_longitude,
    route1:route11,
    entry,
    vending,
    entrance_type,
    ada
  ) |>
  mutate(entry = case_match(entry,
                            "YES" ~ TRUE,
                            "NO" ~ FALSE))
```

This dataset contains data on each entrance and exit for each subway line and station (name and geographic location), and the routes that operate at the line and station (for example, some stops accomodate multiple trains on the same line, like the NQRW or the 123). There are additional variables indicating the entrance type, presence of vending machines, and ADA compliance.

So far, I imported the data using `read_csv`, cleaned up the column names using `janitor::clean_names`, selected the relevant columns using `select`, and transformed the `entry` column from character to logical. After these steps, there are `r nrow(subway_df)` rows and `r ncol(subway_df)` columns. 

There are `r subway_df |> distinct(line,station_name) |> nrow()` distinct stations. Of those, `r subway_df |> filter(ada == TRUE) |> distinct(line,station_name) |> nrow()` are ADA compliant. Of the station entrances/exits without vending, `r subway_df |> filter(vending == "NO") |> summarize(sum(entry)) |> pull()` out of `r subway_df |> filter(vending == "NO") |> nrow()` allow entrance, or `r subway_df |> filter(vending == "NO") |> summarize(100*sum(entry)/n()) |> pull() |> round(1)` percent. 


Now, I will reformat the data from wide to long, into proper tidy format:
```{r}
subway_df <- subway_df |>
  mutate(across(.cols = route8:route11,
                as.character)) |>
  pivot_longer(cols = route1:route11,
               names_to = "route_number",
               values_to = "route_name",
               names_prefix = "route") |>
  drop_na(route_name)
```

#### How many distinct stations serve the A train? 
There are `r subway_df |> filter(route_name == "A") |> distinct(line,station_name) |> nrow()` distinct stations that serve the A train. Of those, `r subway_df |> filter(route_name == "A") |> distinct(line,station_name,ada) |> summarize(sum(ada))` are ADA compliant.



## Problem 2

First we have to read in and clean the data for Mr. Trash Wheel. Then we will follow a similar process for Professor Trash Wheel and Gwynnda, and eventually combine them all into one tidy data set. First, Mr. Trash Wheel:

```{r}
mr_trash_wheel <- read_excel("data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = "Mr. Trash Wheel",
                             range = "A2:N586") |>
  janitor::clean_names() |>
  mutate(sports_balls = as.integer(sports_balls),
         year = as.numeric(year),
         name = "mr_trash_wheel") 
```

And repeat the same process for Professor Trash Wheel and Gwynnda Trash Wheel:
```{r}
prof_trash_wheel <- read_excel("data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = "Professor Trash Wheel",
                             range = "A2:M108") |>
  janitor::clean_names() |>
  mutate(name = "prof_trash_wheel")

gwyn_trash_wheel <- read_excel("data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = "Gwynnda Trash Wheel",
                             range = "A2:L157") |>
  janitor::clean_names() |>
  mutate(name = "gwyn_trash_wheel")
```

Now, combining all the datasets:

```{r}
trash_wheels <- bind_rows(mr_trash_wheel,prof_trash_wheel,gwyn_trash_wheel)
```

With these 3 trash wheel datasets combined, there are `r nrow(trash_wheels)` rows (`r nrow(mr_trash_wheel)` for Mr. Trash Wheel, `r nrow(prof_trash_wheel)` for Professor Trash Wheel, and `r nrow(gwyn_trash_wheel)` for Gwynnda Trash Wheel) and `r ncol(trash_wheels)` columns. Observations range from `r min(pull(trash_wheels, date))` to `r max(pull(trash_wheels,date))`. Inspecting the data carefully, that date from 1900 appears to be an error in the original source excel sheet, not an issue in the data import process, so it could be worth asking the original data producer what that date should be. It appears to be a date in January 2020, but it's unclear which day. The true earliest date in the dataset is `r trash_wheels |> filter(year == year(date)) |> pull(date) |> min()`.

The total weight of trash collected by Professor Trash wheel was `r trash_wheels |> filter(name == "prof_trash_wheel") |> pull(weight_tons) |> sum()` tons over a period from `r min(pull(prof_trash_wheel, date))` to `r max(pull(prof_trash_wheel,date))`.

The total number of cigarette butts collected by Gwynnda in June, 2022 was `r trash_wheels |> filter(name == "gwyn_trash_wheel", month == "June", year == 2022) |> pull(cigarette_butts) |> sum() |> format(scientific = FALSE)`.

## Problem 3

First, I will import the 3 datasets, and clean up the names in the process.
```{r}
bakers_df <- read_csv("data/bakers.csv") |>
  janitor::clean_names() 

bakes_df <- read_csv("data/bakes.csv") |>
  janitor::clean_names()

results_df <- read_csv("data/results.csv",
                       skip = 2) 
```
`results.csv` already has clean names, but I had to skip the first two rows which had descriptive data that wasn't part of the table.

Using `view` and printing the datasets, they look generally complete. One initial problem I noticed is that the `baker` column in `bakes_df` and `results_df` using first name only, while the `bakers_df` using first and last name. 

Transforming that column in `bakers_df`
```{r}
bakers_df <- bakers_df |>
  mutate(baker = str_split_i(baker_name," ",1))
```

Now I'll double check for completeness using `anti_join`, on `baker` and `series` (to ensure that people with same first name aren't matched).
```{r}
anti_join(bakers_df, bakes_df, by = c("baker", "series"))
anti_join(bakes_df, bakers_df, by = c("baker", "series"))

anti_join(bakers_df,results_df, by = c("baker","series"))
anti_join(results_df,bakers_df, by = c("baker","series"))

anti_join(bakes_df,results_df, by = c("baker","series"))
anti_join(results_df,bakes_df, by = c("baker","series"))
```

Looking at these results, two things jump out:
1. The baker named "Joanne" in series 2 is not matched to a baker in `bakers_df`. Instead there is a baker named "Jo" in `bakers_df`, who appears to be the same person.
2. `bakes_df` does not contain any bakes from series 9 or 10, so all the bakers and results from those series are not present.

The first problem is fixable, the second is not; we will have to merge the data without the bakes from seasons 9 and 10.

Now merging the data:
```{r}
gbbo_df <- bakers_df |>
  mutate(baker = case_when(series == 2 & baker == "Jo" ~ "Joanne",
                           .default = baker)) |>
  full_join(bakes_df, by = c("series", "baker")) |>
  full_join(results_df, by = c("series", "baker", "episode")) |>
  arrange(series, episode) |>
  select(series,
         episode,
         baker,      
         result,
         signature_bake,
         technical,
         show_stopper,
         everything())
```

Export to CSV
```{r}
write_csv(gbbo_df,"data/gbbo_data.csv")
```


Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset.

Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table â€“ were there any predictable overall winners? Any surprises?

Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. What was the average viewership in Season 1? In Season 5?



# Conclusion